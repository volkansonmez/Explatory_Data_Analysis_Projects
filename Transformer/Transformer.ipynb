{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transformer encoder with pytorch to be used for sentiment analysis on the IMDB dataset\n",
    "# 25000 comments on IMDB will be fed into an encoder to see if the comments are postive or negative. \n",
    "# this is an enhanced notebook that is originally derived from the NYU Deep Learning Class 2020\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f912249ce30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device to use GPU if available \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Allow randomness just once running this notebook\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the torchtext to access the dataset\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Set the lemgth of each comment (word per instances) made at the IMDB dataset. \n",
    "max_len = 200 # max number of words for each instance \n",
    "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
    "label = data.LabelField(sequential=False, dtype=torch.long)\n",
    "datasets.IMDB.download('./') # download the dataset to the working folder\n",
    "ds_train, ds_test = datasets.IMDB.splits(text, label, path='./imdb/aclImdb/') # split the data into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  25000\n",
      "test :  25000\n",
      "train.fields : {'text': <torchtext.data.field.Field object at 0x7f9123d44d10>, 'label': <torchtext.data.field.LabelField object at 0x7f9123c72e90>}\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the training and test sets\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  22500\n",
      "valid :  2500\n",
      "test :  25000\n",
      "['if', 'you', 'have', 'not', 'heard', 'of', 'this', 'film', 'from', 'walt', 'disney', 'pictures,', 'do', 'not', 'worry', 'about', 'it.', 'it', 'would', 'be', 'classed', 'along', 'the', 'other', 'films', 'by', 'disney', 'that', 'are', 'meant', 'for', 'educational', 'purposes', 'like', '\"family', 'planning\".<br', '/><br', '/>it', 'was', 'co-produced', 'with', 'kotex', 'to', 'teach', 'pre-teen', 'girls', 'about', 'menstruation,', 'supposably.', 'it', 'only', 'educates', 'at', 'a', 'superficial', 'level,', 'so', 'it', 'does', 'not', 'go', 'into', 'heavy', 'detail', 'for', 'the', 'animated', '\"ram\\'s', 'head\"/', 'reproductive', 'system', 'sequence.<br', '/><br', '/>the', 'film', 'does', 'show', '\"the', 'wonderful', 'world', 'of', 'disney\"', 'elements', 'like', 'the', 'turning', 'of', 'the', 'page', 'and', 'the', 'use', 'of', 'animation', 'to', 'tell', 'the', 'story.<br', '/><br', '/>this', 'film', 'is', 'impossible', 'to', 'find,', 'so', 'if', 'you', 'can', 'find', 'the', 'film,', 'best', 'luck', 'to', 'you', 'and', 'enjoy.'] 118\n"
     ]
    }
   ],
   "source": [
    "# Split the training data (90-10) into training data and validation data\n",
    "ds_train, ds_valid = ds_train.split(0.9)\n",
    "print('train : ', len(ds_train))\n",
    "print('valid : ', len(ds_valid))\n",
    "print('test : ', len(ds_test))\n",
    "\n",
    "# View one of the instances of the training data\n",
    "print(ds_train[0].text, len(ds_train[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each instance in train set into an array (each word is a value 0-50000) and train labels into 0 or 1\n",
    "# the word capacity we choose is 50K to name each word a number (max number of words)\n",
    "num_words = 50_000  \n",
    "\n",
    "# since we picked max length 200 words for each instance, the output of each instance is an array with max 200 elements\n",
    "# if an instance has less than 200 words, the remaining of the elements will be \"1\" when it is numericalized\n",
    "text.build_vocab(ds_train, max_size=num_words) # at this point, text data is numericalized, each word is now a number\n",
    "label.build_vocab(ds_train) # label data is numericalized as well (neg:0 or pos:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Regroup instances under batches, each batch having 64 instances (a value between 32-500 is a good choice)\n",
    "batch_size = 64\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volkansonmez/miniconda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Turn the train_loader batch into an iterable format \n",
    "train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "for batch in train_iterator: # everytime it is called, a new batch is called with 64 instances each having 200 values  \n",
    "    x = batch.text.to(device)\n",
    "    y = batch.label.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1426,   209,   859,   781,     6,    26,     3,   104,  1517,    50,\n",
      "            9,    14,     3,  3613,  1426,    10,    20,    14, 17271,     6,\n",
      "          477,     9,   197,    12,    14,   376, 19984,    30,     5,     2,\n",
      "          115,   122,     9,    62,  4998,    13,  4739,    44,     3,   104,\n",
      "         2452,    18,   209,   756,    45,    25,   460,     0,     0,    25,\n",
      "          128,    78,  1256,    26,   301,    17,     0,     0,    10,    14,\n",
      "            2,    20,   178, 37471,  7557,    57,     0,    58,    10,    30,\n",
      "            7,   157,   821,    37,     9,  1107,    44,    39,     3,    56,\n",
      "          291,    12,  1544,    28,   545,  1090,     4,    45,    25,   460,\n",
      "            2,    82,   647,    25,   128,    37,     0,     0,    44,    39,\n",
      "            3,   233,    20,    17,    46,  1047,   811,    16,   111,  4074,\n",
      "         8951,   110,   205,   341,  3462,     4,   363,    11,    44,     3,\n",
      "          838,  1307,   188,  2509,    13,  8320,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]) torch.Size([64, 200])\n",
      "tensor([1, 0, 0, 0, 1]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Check one single instance of a typical batch\n",
    "print(x[13], x.shape) # x torch.Size([64, 200])\n",
    "print(y[0:5], y.shape) # y torch.Size([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(4, 8)\n",
      "tensor([[ 0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, -0.8437]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# See how nn.Embedding layer works\n",
    "\n",
    "# first step is to encode these numericalized arrays in our dataset, adding each value a dimension \n",
    "# nn.Embedding is a linear layer (M x N matrix), with M: number of words and N: size of each word vector.\n",
    "# multiplying a one-hot vector with the embedding outputs the embedded item, in other words, embedding is a look up.\n",
    "\n",
    "# word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "# position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "# there is also sinusoidal embeddings needed to be added to word + positional embeddings for large problems\n",
    "\n",
    "# Example for a basic embedding:\n",
    "word_to_ix = {\"welcome\": 0, \"to\": 1, \"pythonic\": 2, \"fool\": 3}\n",
    "embeds = nn.Embedding(4, 8)  # 4 words in vocab, a new dimension for each word is created with len:8\n",
    "print(embeds)\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"fool\"]], dtype=torch.long)\n",
    "embeded_value = embeds(lookup_tensor)\n",
    "print(embeded_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5],\n",
      "        [0, 1, 2, 3, 4, 5],\n",
      "        [0, 1, 2, 3, 4, 5]]) torch.Size([3, 6]) \n",
      "\n",
      "torch.Size([3, 6, 5])\n",
      "tensor([[[-0.6136,  0.0316, -0.4927,  0.2484,  0.4397],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2897,  0.0525,  0.5229,  2.3022, -1.4689],\n",
      "         [-1.5867, -0.6731,  0.8728,  1.0554,  0.1778],\n",
      "         [-0.2303, -0.3918,  0.5433, -0.3952, -0.4462],\n",
      "         [ 0.7440,  1.5210,  3.4105, -1.5312, -1.2341]],\n",
      "\n",
      "        [[-0.6136,  0.0316, -0.4927,  0.2484,  0.4397],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2897,  0.0525,  0.5229,  2.3022, -1.4689],\n",
      "         [-1.5867, -0.6731,  0.8728,  1.0554,  0.1778],\n",
      "         [-0.2303, -0.3918,  0.5433, -0.3952, -0.4462],\n",
      "         [ 0.7440,  1.5210,  3.4105, -1.5312, -1.2341]],\n",
      "\n",
      "        [[-0.6136,  0.0316, -0.4927,  0.2484,  0.4397],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2897,  0.0525,  0.5229,  2.3022, -1.4689],\n",
      "         [-1.5867, -0.6731,  0.8728,  1.0554,  0.1778],\n",
      "         [-0.2303, -0.3918,  0.5433, -0.3952, -0.4462],\n",
      "         [ 0.7440,  1.5210,  3.4105, -1.5312, -1.2341]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# See how positional embedding layer works\n",
    "\n",
    "length = 6\n",
    "a = torch.ones([3,6], dtype = torch.long)\n",
    "position_ids = torch.arange(length, dtype=torch.long) # (max_seq_length)\n",
    "position_ids = position_ids.unsqueeze(0).expand_as(a) # (bs, max_seq_length)\n",
    "print(position_ids, position_ids.shape, '\\n')\n",
    "\n",
    "vocab_size = 100\n",
    "d_model_test = 5\n",
    " # nn.Embedding takes a 2d and outputs 3d: Takes (len,dim) and outputs (len,dim,model) with a vocabulary embedding. \n",
    "embedding_layer = nn.Embedding(vocab_size, d_model_test, padding_idx=1)\n",
    "pos_embeddings = embedding_layer(position_ids) \n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings Class\n",
    "\n",
    "# batch size: 64, max sequence length: 200, vocab_size: 50000, max_pos_embed: 10000, d_model: 32 will be chosen \n",
    "# p attr. is also needed if an additional sinusoidal embedding is used\n",
    "\n",
    "batch_size = 64\n",
    "d_model = 32\n",
    "max_position_embeddings = 10000\n",
    "vocab_size = 50000\n",
    "max_seq_length = 200 # (also actual seq_length since all sequences are padded to 200 words per instance already)\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module): \n",
    "    '''takes an input with dimensions: [bs, max_seq_lenght] and outputs embeddings: [bs, max_seq_length, d_model]'''\n",
    "    \n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings): \n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids): # each input_ids has 64 instances, each instance has 200 values size: [64,200]\n",
    "        seq_length = input_ids.size(1) # size: (batch size, seq length)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long) # (seqmax_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids) # (bs, max_seq_length)\n",
    "        \n",
    "        # prepare the embeddings\n",
    "        word_embeddings = self.word_embeddings(input_ids) # shape: (bs, seq_length, d_model)\n",
    "        position_embeddings = self.position_embeddings(position_ids) # shape: (bs, seq_length, d_model)\n",
    "        embeddings = word_embeddings + position_embeddings # depending on the problem, some coefficients can be used           \n",
    "        normalized_embeddings = self.LayerNorm(embeddings) # normalize the values in the embeddings layers\n",
    "        \n",
    "        # output dimensions are: [bs, max_seq_length, d_model]\n",
    "        return normalized_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Class\n",
    "# num_heads = 8\n",
    "\n",
    "class MultiHeadAttention(nn.Module):  \n",
    "    '''Takes an input with dimensions: [bs, max_seq_lenght, d_model]. Does the dot product with WQ,WK,WV layers. \n",
    "       Splits these values into multiple attention heads. Makes the calculation for scaled dot product attention. \n",
    "       Combines the attention heads into the original dimensions. Passes the result from a linear layer.'''\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_input=None):  # a \"p\" positional embedding vector could be added here \n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads  # dimension per head\n",
    "        \n",
    "        # Initialize the q,k,v matrices\n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False) # shape: [d_model, d_model]\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): # queries, keys, and values will be learned  \n",
    "        batch_size, seq_length, d_model = x.size() # embedded inputs have 3 dimensions\n",
    "        \n",
    "        # self.W_q(x) will yield a matrix with size (batch_size, seq_length, d_model) same shape of the input\n",
    "        W_q_x = self.W_q(x) # (batch_size, seq_length, d_model) Ex: torch.Size([64, 200, 32]) \n",
    "        W_k_x = self.W_k(x)\n",
    "        W_v_x = self.W_v(x)                      \n",
    "        \n",
    "        # split heads [batch_size, seq_length, d_model] --> [batch_size, n_heads, seq_length, d_k]\n",
    "        Q = W_q_x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (batch_size, n_heads, seq_length, d_k)\n",
    "        K = W_q_x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # Ex: torch.Size([64, 8, 200, 4])\n",
    "        V = W_q_x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) \n",
    "        \n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Regroup the heads\n",
    "        H_cat = H_cat.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat) # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        \n",
    "        # Scale the value by d_k so that the soft(arg)max doesnt saturate\n",
    "        Q = Q / np.sqrt(self.d_k)                         # (batch_size, n_heads, seq_length, d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))        # K.T == (batch_size, n_heads, d_k, seq_length)\n",
    "        \n",
    "        # scores will have a dimension of: (bs, n_heads, seq_length, seq_length) where q_length == k_length == max_seq\n",
    "        A = nn.Softmax(dim=-1)(scores)   # (bs, n_heads, seq_length, seq_length) over the last column \n",
    "        \n",
    "        # (bs, n_heads, seq_length, seq_length).(batch_size, n_heads, seq_length, d_k) = (bs, n_heads, seq_length, d_k)\n",
    "        H = torch.matmul(A, V) # (bs, n_heads, seq_length, d_k)\n",
    "\n",
    "        return H, A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of the values that will be used in this model\n",
    "\n",
    "# hidden_dim = 300\n",
    "# batch_size = 64\n",
    "# d_model = 32\n",
    "# num_heads = 8\n",
    "# max_position_embeddings = 10000\n",
    "# vocab_size = 50000\n",
    "# max_seq_length = 200 # (also actual seq_length since all sequences are padded to 200 words per instance already)\n",
    "# no p value is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Encoder Class and a Two-Layer NN \n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.L1 = nn.Linear(d_model,hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L1(x)\n",
    "        # x = nn.ReLU(x) # raises error since torch can't locate the dimensions of where to apply ReLU\n",
    "        x = self.L2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, input_vocab_size, maximum_position_encoding):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = Embeddings(d_model, input_vocab_size, maximum_position_encoding) # init Embeddings object\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads) # init MultiHeadAttention object\n",
    "        self.NN = TwoLayerNN(d_model, hidden_dim) # init TwoLayeNN object\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape = d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape = d_model, eps=1e-6)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform the inputs to embedded values (batch_size, input_seq_length, d_model)\n",
    "        x = self.embedding(x) \n",
    "        \n",
    "        # input the embedded values to Multi Head Attn.\n",
    "        attn_output, _ = self.mha(x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # do layer normalization after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # feed forward the attention+residual connection to a 2-layer-NN\n",
    "        output = self.NN(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # do layer normalization after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # return the transformer's encoder output\n",
    "        return out2  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to transform the ouputs of the encoder to match the labels (similar to binary logistic regression)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, input_vocab_size, maximum_position_encoding=10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(d_model, num_heads, hidden_dim, input_vocab_size,\n",
    "                         maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, 2) # answers are either 0 or 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  \n",
    "        x, _ = torch.max(x, dim=1)  # torch max returns: (max values, argmax values)\n",
    "        x = self.dense(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(10000, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (NN): TwoLayerNN(\n",
       "      (L1): Linear(in_features=32, out_features=400, bias=True)\n",
       "      (L2): Linear(in_features=400, out_features=32, bias=True)\n",
       "    )\n",
       "    (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "    (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model object\n",
    "\n",
    "model = TransformerClassifier(d_model=32, num_heads=8, hidden_dim=400, input_vocab_size=50002, \n",
    "                              maximum_position_encoding =10000)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer and training epochs \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train(train_loader, valid_loader):\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        for batch in train_iterator: # 🤪\n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "            \n",
    "            out = model(x)  \n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(out, y)  \n",
    "            \n",
    "            model.zero_grad()  \n",
    "\n",
    "            loss.backward()  \n",
    "            losses += loss.item()\n",
    "\n",
    "            optimizer.step()  \n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        sum_valid_acc = 0   \n",
    "        for batch in valid_iterator:\n",
    "            model.eval()\n",
    "            acc = 0 \n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "                \n",
    "            out = model(x)\n",
    "            sum_valid_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "            valid_acc = sum_valid_acc / len(valid_loader)\n",
    "        \n",
    "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
    "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
    "        print('Evaluating on validation:' , valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 1 is 0.6719875825061039\n",
      "Training accuracy: 0.5813703440656566\n",
      "Evaluating on validation: 0.640625\n",
      "Training loss at epoch 2 is 0.6052326389842413\n",
      "Training accuracy: 0.6760130602904041\n",
      "Evaluating on validation: 0.685546875\n",
      "Training loss at epoch 3 is 0.5212189660153606\n",
      "Training accuracy: 0.7437756470959596\n",
      "Evaluating on validation: 0.734765625\n",
      "Training loss at epoch 4 is 0.4232503722675822\n",
      "Training accuracy: 0.8075185448232323\n",
      "Evaluating on validation: 0.771875\n",
      "Training loss at epoch 5 is 0.331117995633659\n",
      "Training accuracy: 0.8587042297979799\n",
      "Evaluating on validation: 0.790625\n",
      "Training loss at epoch 6 is 0.25776643604463473\n",
      "Training accuracy: 0.897189670138889\n",
      "Evaluating on validation: 0.81015625\n",
      "Training loss at epoch 7 is 0.20072476316074078\n",
      "Training accuracy: 0.9246221985479799\n",
      "Evaluating on validation: 0.829296875\n",
      "Training loss at epoch 8 is 0.1499159182421863\n",
      "Training accuracy: 0.9470190183080809\n",
      "Evaluating on validation: 0.834765625\n",
      "Training loss at epoch 9 is 0.11008758023275401\n",
      "Training accuracy: 0.9627426609848485\n",
      "Evaluating on validation: 0.826953125\n",
      "Training loss at epoch 10 is 0.07761386958141388\n",
      "Training accuracy: 0.9763158933080809\n",
      "Evaluating on validation: 0.842578125\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8175271739130434\n"
     ]
    }
   ],
   "source": [
    "# Test the model's accuracy\n",
    "\n",
    "test_iterator = iter(test_loader)  \n",
    "nb_batches = len(test_loader)\n",
    "model.eval()\n",
    "sum_test_acc = 0 \n",
    "    \n",
    "for batch in iter(test_loader):\n",
    "    x = batch.text.to(device)\n",
    "    y = batch.label.to(device)\n",
    "    out = model(x)\n",
    "    sum_test_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "print(f\"test accuracy: {sum_test_acc/ nb_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not too bad. The model's accuracy can be further improved by stacking more layers and training it longer time. \n",
    "# The model is an overfit on the training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
